# -*- coding: utf-8 -*-
"""lda.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ULCsbsSO_ehYAad8soj8JthrRT3nH5WE
"""

import numpy as np

class LDA:
    def __init__(self, n_components):
        self.n_components = n_components
        self.linear_discriminants = None

    def fit(self, X, y):
        n_features = X.shape[1]
        class_labels = np.unique(y)
        mean_overall = np.mean(X, axis=0)
        SW = np.zeros((n_features, n_features))
        SB = np.zeros((n_features, n_features))
        for c in class_labels:
            X_c = X[y == c]
            mean_c = np.mean(X_c, axis=0)

            SW += (X_c - mean_c).T.dot((X_c - mean_c))


            n_c = X_c.shape[0]
            mean_diff = (mean_c - mean_overall).reshape(n_features, 1)
            SB += n_c * (mean_diff).dot(mean_diff.T)

        # Determine SW^-1 * SB
        A = np.linalg.inv(SW).dot(SB)
        # Get eigenvalues and eigenvectors of SW^-1 * SB
        eigenvalues, eigenvectors = np.linalg.eig(A)
        # -> eigenvector v = [:,i] column vector, transpose for easier calculations
        # sort eigenvalues high to low
        eigenvectors = eigenvectors.T
        idxs = np.argsort(abs(eigenvalues))[::-1]
        eigenvalues = eigenvalues[idxs]
        eigenvectors = eigenvectors[idxs]
        # store first n eigenvectors
        self.linear_discriminants = eigenvectors[0:self.n_components]
        # We first make a list of (eigenvalue, eigenvector) tuples

        eig_vals, eig_vecs = np.linalg.eigh(A)



        max_abs_idx = np.argmax(np.abs(eig_vecs), axis=0)
        signs = np.sign(eig_vecs[max_abs_idx, range(eig_vecs.shape[0])])
        eig_vecs = eig_vecs*signs[np.newaxis,:]
        eig_vecs = eig_vecs.T


        print(eig_vecs.shape)
        eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[i,:]) for i in range(len(eig_vals))]

        # Then, we sort the tuples from the highest to the lowest based on eigenvalues magnitude
        eig_pairs.sort(key=lambda x: x[0], reverse=True)

        # For further usage
        eig_vals_sorted = np.array([x[0] for x in eig_pairs])
        eig_vecs_sorted = np.array([x[1] for x in eig_pairs])
        eig_vals_total = sum(eig_vals)
        explained_variance = [(i / eig_vals_total)*100 for i in eig_vals_sorted]
        explained_variance = np.round(explained_variance, 2)
        cum_explained_variance = np.cumsum(explained_variance)

        print('Explained variance: {}'.format(explained_variance))
        print('Cumulative explained variance: {}'.format(cum_explained_variance))

        plt.plot(np.arange(1,n_features+1), cum_explained_variance, '-o')
        plt.xticks(np.arange(1,n_features+1))
        plt.xlabel('Number of components')
        plt.ylabel('Cumulative explained variance');
        plt.show()

    def transform(self, X):
        # project data
        return np.dot(X, self.linear_discriminants.T)

from sklearn.datasets import load_wine
import matplotlib.pyplot as plt

data_set= load_wine()
X = data_set['data']
y = data_set['target']

n_samples, n_features = X.shape

print('Number of samples:', n_samples)
print('Number of features:', n_features)

lda = LDA(2)
lda.fit(X, y)
X_Projetado = lda.transform(X)
print('Shape de X:', X.shape)
print('Shape X_Projetado:', X_Projetado.shape)

x1 = X_Projetado[:,0]
x2 = X_Projetado[:,1]

plt.scatter(x1, x2, c=y, edgecolor='none', alpha=0.7, cmap=plt.cm.get_cmap('viridis', 3))
plt.xlabel('Descriminante Linear 1')
plt.ylabel('Descriminante Linear 2')
plt.colorbar()
plt.show()

from sklearn.datasets import load_diabetes

data_set= load_diabetes()
X = data_set['data']
y = data_set['target']

n_samples, n_features = X.shape

print('Number of samples:', n_samples)
print('Number of features:', n_features)

lda = LDA(2)
lda.fit(X, y)
X_Projetado = lda.transform(X)
print('Shape de X:', X.shape)
print('Shape X_Projetado:', X_Projetado.shape)

x1 = X_Projetado[:,0]
x2 = X_Projetado[:,1]

plt.scatter(x1, x2, c=y, edgecolor='none', alpha=0.7, cmap=plt.cm.get_cmap('viridis', 3))
plt.xlabel('Descriminante Linear 1')
plt.ylabel('Descriminante Linear 2')
plt.colorbar()
plt.show()

from sklearn.datasets import load_breast_cancer

data_set= load_breast_cancer()
X = data_set['data']
y = data_set['target']

n_samples, n_features = X.shape

print('Number of samples:', n_samples)
print('Number of features:', n_features)

lda = LDA(2)
lda.fit(X, y)
X_Projetado = lda.transform(X)
print('Shape de X:', X.shape)
print('Shape X_Projetado:', X_Projetado.shape)

x1 = X_Projetado[:,0]
x2 = X_Projetado[:,1]

plt.scatter(x1, x2, c=y, edgecolor='none', alpha=0.7, cmap=plt.cm.get_cmap('viridis', 3))
plt.xlabel('Descriminante Linear 1')
plt.ylabel('Descriminante Linear 2')
plt.colorbar()
plt.show()